#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Falcoæ•°æ®é‡‡é›†åˆ†æè„šæœ¬

è¯¥è„šæœ¬ç”¨äºåˆ†æFalcoæ•°æ®é‡‡é›†çš„æ€§èƒ½å’Œå®Œæ•´æ€§ï¼ŒåŒ…æ‹¬ï¼š
1. è¯»å–é¢‘ç‡åˆ†æ
2. æ•°æ®é‡ç»Ÿè®¡
3. é‡å¤æ•°æ®æ£€æµ‹
4. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
5. ä¼˜åŒ–å»ºè®®ç”Ÿæˆ

ä½œè€…: Falco AI Security Team
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¶é—´: 2024-05-15
"""

import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from app.services.falco_data_integrity import FalcoDataIntegrityService
from app.services.falco_monitor import FalcoMonitorService
from app.config_simple import settings

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FalcoDataAnalyzer:
    """Falcoæ•°æ®é‡‡é›†åˆ†æå™¨"""
    
    def __init__(self):
        # ä½¿ç”¨å®¿ä¸»æœºä¸Šçš„å®é™…æ—¥å¿—æ–‡ä»¶è·¯å¾„
        self.log_file_path = "/home/xzj/01_Project/B_25OS_falco_v3/falco-ai-security/logs/falco.log"
        self.analysis_duration = 15   # åˆ†ææŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
        self.sample_interval = 1      # é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        
        # åˆ†æç»“æœ
        self.analysis_results = {
            'start_time': None,
            'end_time': None,
            'duration': 0,
            'file_stats': {},
            'read_performance': {},
            'data_integrity': {},
            'recommendations': []
        }
    
    def analyze_file_stats(self) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not os.path.exists(self.log_file_path):
                return {'error': f'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.log_file_path}'}
            
            stat = os.stat(self.log_file_path)
            
            # è®¡ç®—æ–‡ä»¶å¢é•¿é€Ÿç‡
            file_size = stat.st_size
            mtime = stat.st_mtime
            
            # è¯»å–æœ€è¿‘çš„å‡ è¡Œæ¥ä¼°ç®—äº‹ä»¶é¢‘ç‡
            recent_events = self._sample_recent_events(100)
            
            return {
                'file_path': self.log_file_path,
                'file_size': file_size,
                'file_size_mb': round(file_size / 1024 / 1024, 2),
                'last_modified': datetime.fromtimestamp(mtime).isoformat(),
                'recent_events_count': len(recent_events),
                'estimated_events_per_minute': self._estimate_event_rate(recent_events)
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _sample_recent_events(self, count: int = 100) -> List[Dict]:
        """é‡‡æ ·æœ€è¿‘çš„äº‹ä»¶"""
        try:
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
                f.seek(0, 2)
                file_size = f.tell()
                
                # ä¼°ç®—éœ€è¦è¯»å–çš„å­—èŠ‚æ•°ï¼ˆå‡è®¾æ¯è¡Œå¹³å‡500å­—èŠ‚ï¼‰
                estimated_bytes = count * 500
                start_pos = max(0, file_size - estimated_bytes)
                
                f.seek(start_pos)
                lines = f.readlines()
                
                # è§£æJSONäº‹ä»¶
                events = []
                for line in lines[-count:]:
                    line = line.strip()
                    if line and line.startswith('{'):
                        try:
                            event = json.loads(line)
                            events.append(event)
                        except json.JSONDecodeError:
                            continue
                
                return events
                
        except Exception as e:
            logger.error(f"é‡‡æ ·æœ€è¿‘äº‹ä»¶å¤±è´¥: {e}")
            return []
    
    def _estimate_event_rate(self, events: List[Dict]) -> float:
        """ä¼°ç®—äº‹ä»¶é¢‘ç‡ï¼ˆæ¯åˆ†é’Ÿï¼‰"""
        if len(events) < 2:
            return 0.0
        
        try:
            # è·å–æ—¶é—´æˆ³
            timestamps = []
            for event in events:
                if 'time' in event:
                    # ISOæ ¼å¼æ—¶é—´æˆ³
                    ts = datetime.fromisoformat(event['time'].replace('Z', '+00:00'))
                    timestamps.append(ts.timestamp())
                elif 'timestamp' in event:
                    # Unixæ—¶é—´æˆ³
                    timestamps.append(float(event['timestamp']))
            
            if len(timestamps) < 2:
                return 0.0
            
            # è®¡ç®—æ—¶é—´è·¨åº¦
            time_span = max(timestamps) - min(timestamps)
            if time_span <= 0:
                return 0.0
            
            # è®¡ç®—æ¯åˆ†é’Ÿäº‹ä»¶æ•°
            events_per_second = len(timestamps) / time_span
            return round(events_per_second * 60, 2)
            
        except Exception as e:
            logger.error(f"ä¼°ç®—äº‹ä»¶é¢‘ç‡å¤±è´¥: {e}")
            return 0.0
    
    async def analyze_read_performance(self) -> Dict[str, Any]:
        """åˆ†æè¯»å–æ€§èƒ½"""
        logger.info("å¼€å§‹åˆ†æè¯»å–æ€§èƒ½...")
        
        start_time = time.time()
        read_stats = []
        
        # ç›‘æ§è¯»å–æ€§èƒ½ - ç›´æ¥ä»æ–‡ä»¶è¯»å–æœ€è¿‘äº‹ä»¶è¿›è¡Œæµ‹è¯•
        for i in range(self.analysis_duration // self.sample_interval):
            sample_start = time.time()
            
            # ç›´æ¥è¯»å–æœ€è¿‘çš„äº‹ä»¶è¿›è¡Œæ€§èƒ½æµ‹è¯•
            events = self._sample_recent_events(50)  # æ¯æ¬¡è¯»å–50ä¸ªæœ€è¿‘äº‹ä»¶
            
            sample_end = time.time()
            read_time = sample_end - sample_start
            
            read_stats.append({
                'timestamp': sample_start,
                'read_time': read_time,
                'events_count': len(events),
                'events_per_second': len(events) / read_time if read_time > 0 else 0
            })
            
            logger.info(f"æ ·æœ¬ {i+1}/{self.analysis_duration // self.sample_interval}: "
                       f"è¯»å– {len(events)} ä¸ªäº‹ä»¶ï¼Œè€—æ—¶ {read_time:.3f}s")
            
            await asyncio.sleep(self.sample_interval)
        
        end_time = time.time()
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_events = sum(stat['events_count'] for stat in read_stats)
        total_read_time = sum(stat['read_time'] for stat in read_stats)
        avg_read_time = total_read_time / len(read_stats) if read_stats else 0
        avg_events_per_read = total_events / len(read_stats) if read_stats else 0
        
        return {
            'analysis_duration': end_time - start_time,
            'total_samples': len(read_stats),
            'total_events_read': total_events,
            'total_read_time': total_read_time,
            'avg_read_time': avg_read_time,
            'avg_events_per_read': avg_events_per_read,
            'max_read_time': max((stat['read_time'] for stat in read_stats), default=0),
            'min_read_time': min((stat['read_time'] for stat in read_stats), default=0),
            'events_per_minute': (total_events / (end_time - start_time)) * 60 if end_time > start_time else 0
        }
    
    async def analyze_data_integrity(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®å®Œæ•´æ€§"""
        logger.info("å¼€å§‹åˆ†ææ•°æ®å®Œæ•´æ€§...")
        
        # åˆ›å»ºæ•°æ®å®Œæ•´æ€§æœåŠ¡å®ä¾‹
        integrity_service = FalcoDataIntegrityService()
        
        # è¿è¡Œå®Œæ•´æ€§éªŒè¯
        validation_result = await integrity_service.validate_data_integrity(
            check_duration=min(self.analysis_duration, 60)
        )
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = integrity_service.get_stats()
        
        return {
            'validation_result': validation_result,
            'service_stats': stats,
            'integrity_score': validation_result.get('integrity_score', 0.0)
        }
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ–‡ä»¶ç»Ÿè®¡çš„å»ºè®®
        file_stats = self.analysis_results.get('file_stats', {})
        if 'estimated_events_per_minute' in file_stats:
            event_rate = file_stats['estimated_events_per_minute']
            
            if event_rate > 1000:
                recommendations.append(
                    f"äº‹ä»¶é¢‘ç‡è¾ƒé«˜ ({event_rate:.1f}/åˆ†é’Ÿ)ï¼Œå»ºè®®å¢åŠ æ‰¹å¤„ç†å¤§å°æˆ–é™ä½å¤„ç†é¢‘ç‡"
                )
            elif event_rate < 10:
                recommendations.append(
                    f"äº‹ä»¶é¢‘ç‡è¾ƒä½ ({event_rate:.1f}/åˆ†é’Ÿ)ï¼Œå¯ä»¥å¢åŠ å¤„ç†é¢‘ç‡ä»¥æé«˜å®æ—¶æ€§"
                )
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
        if file_stats.get('file_size', 0) == 0:
            recommendations.append(
                "Falcoæ—¥å¿—æ–‡ä»¶ä¸ºç©ºï¼Œè¯·æ£€æŸ¥FalcoæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"
            )
        
        # åŸºäºè¯»å–æ€§èƒ½çš„å»ºè®®
        read_perf = self.analysis_results.get('read_performance', {})
        if 'avg_read_time' in read_perf:
            avg_read_time = read_perf['avg_read_time']
            
            if avg_read_time > 0.1:
                recommendations.append(
                    f"å¹³å‡è¯»å–æ—¶é—´è¾ƒé•¿ ({avg_read_time:.3f}s)ï¼Œå»ºè®®ä¼˜åŒ–I/Oæ“ä½œæˆ–å¢åŠ ç¼“å­˜"
                )
            
            if read_perf.get('max_read_time', 0) > avg_read_time * 3:
                recommendations.append(
                    "æ£€æµ‹åˆ°è¯»å–æ—¶é—´æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œç£ç›˜æ€§èƒ½"
                )
        
        # åŸºäºæ•°æ®å®Œæ•´æ€§çš„å»ºè®®
        integrity = self.analysis_results.get('data_integrity', {})
        if 'integrity_score' in integrity:
            score = integrity['integrity_score']
            
            if score < 0.9:
                recommendations.append(
                    f"æ•°æ®å®Œæ•´æ€§å¾—åˆ†è¾ƒä½ ({score:.2f})ï¼Œå»ºè®®æ£€æŸ¥é‡å¤å¤„ç†é€»è¾‘"
                )
        
        # åŸºäºé…ç½®çš„å»ºè®®
        current_interval = getattr(settings, 'FALCO_PROCESSING_INTERVAL', 1)
        current_batch_size = getattr(settings, 'FALCO_MAX_EVENTS_PER_BATCH', 100)
        
        if read_perf.get('avg_events_per_read', 0) < current_batch_size * 0.5:
            recommendations.append(
                f"å®é™…è¯»å–äº‹ä»¶æ•° ({read_perf.get('avg_events_per_read', 0):.1f}) "
                f"è¿œä½äºæ‰¹å¤„ç†å¤§å° ({current_batch_size})ï¼Œå»ºè®®é™ä½æ‰¹å¤„ç†å¤§å°"
            )
        
        return recommendations
    
    async def run_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info(f"å¼€å§‹Falcoæ•°æ®é‡‡é›†åˆ†æï¼ŒæŒç»­æ—¶é—´: {self.analysis_duration}ç§’")
        
        self.analysis_results['start_time'] = datetime.now().isoformat()
        start_time = time.time()
        
        try:
            # 1. åˆ†ææ–‡ä»¶ç»Ÿè®¡
            logger.info("1/3 åˆ†ææ–‡ä»¶ç»Ÿè®¡...")
            self.analysis_results['file_stats'] = self.analyze_file_stats()
            
            # 2. åˆ†æè¯»å–æ€§èƒ½
            logger.info("2/3 åˆ†æè¯»å–æ€§èƒ½...")
            self.analysis_results['read_performance'] = await self.analyze_read_performance()
            
            # 3. åˆ†ææ•°æ®å®Œæ•´æ€§
            logger.info("3/3 åˆ†ææ•°æ®å®Œæ•´æ€§...")
            self.analysis_results['data_integrity'] = await self.analyze_data_integrity()
            
            # ç”Ÿæˆå»ºè®®
            self.analysis_results['recommendations'] = self.generate_recommendations()
            
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.analysis_results['error'] = str(e)
        
        finally:
            end_time = time.time()
            self.analysis_results['end_time'] = datetime.now().isoformat()
            self.analysis_results['duration'] = end_time - start_time
        
        return self.analysis_results
    
    def save_results(self, output_file: str = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'/tmp/falco_analysis_{timestamp}.json'
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            return None
    
    def print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*60)
        print("Falcoæ•°æ®é‡‡é›†åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # æ–‡ä»¶ç»Ÿè®¡
        file_stats = self.analysis_results.get('file_stats', {})
        if 'file_size_mb' in file_stats:
            print(f"\nğŸ“ æ–‡ä»¶ç»Ÿè®¡:")
            print(f"  æ–‡ä»¶å¤§å°: {file_stats['file_size_mb']} MB")
            print(f"  äº‹ä»¶é¢‘ç‡: {file_stats.get('estimated_events_per_minute', 0):.1f} äº‹ä»¶/åˆ†é’Ÿ")
        
        # è¯»å–æ€§èƒ½
        read_perf = self.analysis_results.get('read_performance', {})
        if 'avg_read_time' in read_perf:
            print(f"\nâš¡ è¯»å–æ€§èƒ½:")
            print(f"  å¹³å‡è¯»å–æ—¶é—´: {read_perf['avg_read_time']:.3f}s")
            print(f"  å¹³å‡äº‹ä»¶æ•°/æ¬¡: {read_perf['avg_events_per_read']:.1f}")
            print(f"  äº‹ä»¶å¤„ç†é€Ÿç‡: {read_perf['events_per_minute']:.1f} äº‹ä»¶/åˆ†é’Ÿ")
        
        # æ•°æ®å®Œæ•´æ€§
        integrity = self.analysis_results.get('data_integrity', {})
        if 'integrity_score' in integrity:
            score = integrity['integrity_score']
            print(f"\nğŸ”’ æ•°æ®å®Œæ•´æ€§:")
            print(f"  å®Œæ•´æ€§å¾—åˆ†: {score:.2f}/1.00")
            
            if score >= 0.95:
                print(f"  çŠ¶æ€: ä¼˜ç§€ âœ…")
            elif score >= 0.85:
                print(f"  çŠ¶æ€: è‰¯å¥½ âš ï¸")
            else:
                print(f"  çŠ¶æ€: éœ€è¦æ”¹è¿› âŒ")
        
        # ä¼˜åŒ–å»ºè®®
        recommendations = self.analysis_results.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®: å½“å‰é…ç½®è‰¯å¥½ï¼Œæ— éœ€è°ƒæ•´")
        
        print("\n" + "="*60)


async def main():
    """ä¸»å‡½æ•°"""
    analyzer = FalcoDataAnalyzer()
    
    # è¿è¡Œåˆ†æ
    results = await analyzer.run_analysis()
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()
    
    # ä¿å­˜ç»“æœ
    output_file = analyzer.save_results()
    if output_file:
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())